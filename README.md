# ğŸ”® Prism - Multimodal RAG System

<div align="center">

![Prism Logo](https://img.shields.io/badge/Prism-Multimodal%20RAG-blueviolet?style=for-the-badge&logo=data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjQiIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggZD0iTTEyIDJMMjIgMjBIMloiIGZpbGw9InVybCgjZ3JhZGllbnQpIi8+CjxkZWZzPgo8bGluZWFyR3JhZGllbnQgaWQ9ImdyYWRpZW50IiB4MT0iMCIgeTE9IjAiIHgyPSIxIiB5Mj0iMSI+CjxzdG9wIG9mZnNldD0iMCUiIHN0b3AtY29sb3I9IiM2MzY2ZjEiLz4KPHN0b3Agb2Zmc2V0PSIxMDAlIiBzdG9wLWNvbG9yPSIjOWMzNGY0Ii8+CjwvbGluZWFyR3JhZGllbnQ+CjwvZGVmcz4KPHN2Zz4K)

**Your intelligent multimodal search companion**

[![React](https://img.shields.io/badge/React-18.2.0-61DAFB?style=flat-square&logo=react)](https://reactjs.org/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.104.1-009688?style=flat-square&logo=fastapi)](https://fastapi.tiangolo.com/)
[![Python](https://img.shields.io/badge/Python-3.8+-3776AB?style=flat-square&logo=python)](https://python.org/)
[![License](https://img.shields.io/badge/License-MIT-green?style=flat-square)](LICENSE)

</div>

---

## ğŸŒŸ Overview

Prism is a **multimodal RAG (Retrieval-Augmented Generation) system** that enables intelligent search and question-answering across documents (PDF, DOCX), images, and audio files. Built with a **local-first approach**, all processing happens on-device with no cloud dependencies, ensuring complete privacy and security.

### âœ¨ Key Features

- ğŸ” **Multimodal Search** - Search across documents, images, and audio with natural language
- ğŸ§  **Document Q&A** - Ask questions about uploaded documents using Mistral 7B LLM
- ğŸ  **Local-First** - All processing happens on your device, no cloud dependencies
- ğŸ“„ **Document Processing** - Support for PDF and DOCX files with intelligent chunking
- ğŸ¨ **Modern UI** - Beautiful React interface with Tailwind CSS and Framer Motion
- ğŸš€ **Fast API** - High-performance FastAPI backend with async processing
- ğŸ”’ **Privacy-Focused** - Your data never leaves your device

### ğŸ¯ Current Status

**Implemented Features:**
- âœ… Document upload and processing (PDF/DOCX)
- âœ… Intelligent text chunking with token awareness
- âœ… Document Q&A using Mistral 7B via llama.cpp
- âœ… Modern React frontend with multiple interfaces
- âœ… FastAPI backend with comprehensive endpoints
- âœ… Local LLM integration for offline operation

**In Development:**
- ğŸ”¨ Image OCR and processing
- ğŸ”¨ Audio transcription
- ğŸ”¨ Vector embeddings for semantic search
- ğŸ”¨ Full multimodal search interface

---

## ğŸš€ Quick Start

### Prerequisites

- **Python 3.8+** with pip
- **Node.js 16+** with npm
- **8GB+ RAM** (for running Mistral 7B)
- **4GB+ disk space** (for model storage)

### 1. Clone the Repository

```bash
git clone <repository-url>
cd prism
```

### 2. Set Up the Backend

```bash
cd backend

# Create and activate virtual environment
python -m venv .venv
.venv\Scripts\activate  # Windows
# source .venv/bin/activate  # macOS/Linux

# Install dependencies
pip install -r requirements.txt
```

### 3. Download the LLM Model

```bash
# Create models directory
mkdir -p models/llm

# Download Mistral 7B (4.1GB)
curl -L -o models/llm/mistral-7b-instruct-v0.2.Q4_K_M.gguf https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q4_K_M.gguf
```

### 4. Set Up the Frontend

```bash
cd frontend
npm install
```

### 5. Start the Application

**Terminal 1 (Backend):**
```bash
cd backend
.venv\Scripts\activate
python -m uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

**Terminal 2 (Frontend):**
```bash
cd frontend
npm run dev
```

Open your browser and go to `http://localhost:3000` ğŸ‰

> ğŸ“‹ For detailed setup instructions, see [SETUP_QA.md](SETUP_QA.md)

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   React Frontend â”‚    â”‚  FastAPI Backend â”‚    â”‚ Mistral 7B LLM  â”‚
â”‚   (Port 3000)   â”‚â—„â”€â”€â–ºâ”‚   (Port 8000)   â”‚â—„â”€â”€â–ºâ”‚  (llama.cpp)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
         â–¼                       â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  File Upload    â”‚    â”‚ Document        â”‚    â”‚ Question        â”‚
â”‚  Interface      â”‚    â”‚ Processing      â”‚    â”‚ Answering       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ”§ Tech Stack

**Frontend:**
- React 18 with Vite
- Tailwind CSS for styling
- Framer Motion for animations
- Lucide React for icons
- Axios for API calls

**Backend:**
- FastAPI for high-performance APIs
- llama.cpp for LLM inference
- PyPDF2 & python-docx for document parsing
- tiktoken for intelligent text chunking

**AI/ML:**
- Mistral 7B Instruct v0.2 (GGUF format)
- Token-aware document chunking
- Local inference with no cloud dependencies

---

## ğŸ“ Project Structure

```
prism/
â”œâ”€â”€ ğŸ¨ frontend/                 # React application
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/          # UI components
â”‚   â”‚   â”‚   â”œâ”€â”€ DocumentQA.jsx   # Q&A interface
â”‚   â”‚   â”‚   â”œâ”€â”€ FileUpload.jsx   # File upload UI
â”‚   â”‚   â”‚   â”œâ”€â”€ SearchInterface.jsx
â”‚   â”‚   â”‚   â””â”€â”€ ...
â”‚   â”‚   â”œâ”€â”€ App.jsx              # Main app component
â”‚   â”‚   â””â”€â”€ index.css            # Tailwind styles
â”‚   â””â”€â”€ package.json
â”‚
â”œâ”€â”€ ğŸ”§ backend/                  # FastAPI application
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ main.py              # API endpoints
â”‚   â”‚   â””â”€â”€ services/
â”‚   â”‚       â”œâ”€â”€ llm_service.py   # Mistral LLM integration
â”‚   â”‚       â””â”€â”€ qa_service.py    # Document Q&A logic
â”‚   â”œâ”€â”€ ingestion/
â”‚   â”‚   â”œâ”€â”€ parse_pdf.py         # PDF/DOCX processing
â”‚   â”‚   â””â”€â”€ chunker.py           # Intelligent text chunking
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ ğŸ§  models/
â”‚   â””â”€â”€ llm/                     # LLM model storage
â”‚       â””â”€â”€ mistral-7b-instruct-v0.2.Q4_K_M.gguf
â”‚
â”œâ”€â”€ ğŸ“Š data/                     # Application data
â”‚   â”œâ”€â”€ uploads/                 # Uploaded files
â”‚   â”œâ”€â”€ processed/               # Processed documents
â”‚   â””â”€â”€ indices/                 # Search indices
â”‚
â””â”€â”€ ğŸ“š docs/                     # Documentation
    â”œâ”€â”€ architecture.md
    â””â”€â”€ runbook.md
```

---

## ğŸ¯ Usage

### Document Q&A

1. **Upload Documents** - Upload PDF or DOCX files via the Q&A interface
2. **Processing** - Documents are automatically chunked and indexed
3. **Ask Questions** - Type natural language questions about your documents
4. **Get Answers** - Receive contextual answers with source citations

**Example Questions:**
- "What are the main conclusions in this report?"
- "Summarize the budget section"
- "What recommendations are mentioned?"
- "Find information about project timelines"

### Search Interface

- **Text Search** - Search across all processed documents
- **Multimodal Support** - Future support for image and audio search
- **Smart Results** - Relevance scoring and source citations

---

## ğŸ”§ Configuration

### Model Settings

Edit `backend/app/services/llm_service.py`:

```python
# Context window size (adjust for memory usage)
self.n_ctx = 4096

# Thread count (adjust for CPU cores)
self.n_threads = 4

# Temperature (0.0 = focused, 1.0 = creative)
temperature = 0.7
```

### Chunking Settings

Edit `backend/ingestion/chunker.py`:

```python
# Chunk size in tokens
chunk_size = 1000

# Overlap between chunks
chunk_overlap = 200
```

---

## ğŸ› ï¸ Development

### Frontend Development

```bash
cd frontend
npm run dev     # Start development server
npm run build   # Build for production
npm run lint    # Run ESLint
```

### Backend Development

```bash
cd backend
.venv\Scripts\activate
python -m uvicorn app.main:app --reload  # Start with auto-reload
```

### Testing

```bash
# Run backend tests
cd backend
python -m pytest tests/

# Run frontend tests
cd frontend
npm test
```

---

## ğŸ“Š API Documentation

The FastAPI backend provides comprehensive API documentation:

- **Interactive Docs**: `http://localhost:8000/docs`
- **ReDoc**: `http://localhost:8000/redoc`

### Key Endpoints

| Method | Endpoint | Description |
|--------|----------|-------------|
| `GET` | `/` | Health check and model status |
| `POST` | `/api/upload` | Upload and process documents |
| `POST` | `/api/question` | Ask questions about documents |
| `GET` | `/api/documents` | List processed documents |
| `DELETE` | `/api/documents/{file_id}` | Delete documents |
| `GET` | `/api/model/status` | Check LLM model status |

---

## ğŸ”’ Privacy & Security

- **Local Processing** - All data processing happens on your device
- **No Cloud Dependencies** - Documents never leave your machine
- **Offline Operation** - Works completely offline after setup
- **Data Control** - You own and control all your data

---

## ğŸ¤ Contributing

We welcome contributions! Please read our contributing guidelines:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

---

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ†˜ Support

- **Documentation**: Check [SETUP_QA.md](SETUP_QA.md) for detailed setup
- **Issues**: Report bugs via GitHub Issues
- **Discussions**: Join discussions for questions and ideas

---

<div align="center">

**Built with â¤ï¸ for privacy-focused AI**

[â­ Star this repo](../../stargazers) â€¢ [ğŸ› Report Bug](../../issues) â€¢ [ğŸ’¡ Request Feature](../../issues)

</div>